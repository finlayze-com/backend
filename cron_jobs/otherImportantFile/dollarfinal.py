import os, time, tempfile
import pandas as pd
import psycopg2
from dotenv import load_dotenv

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import ElementClickInterceptedException, TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

# -------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ --------------------
load_dotenv()
URL = "https://www.tgju.org/profile/price_dollar_rl/history"
TABLE_ID = "DataTables_Table_0"

# -------------------- Driver --------------------
def build_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument(f'--user-data-dir={tempfile.mkdtemp()}')
    # Ø§Ú¯Ø± Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± Ù…Ø³ÛŒØ± Ø¨Ø§ÛŒÙ†Ø±ÛŒ Chrome Ø¯Ø§Ø±ÛŒ
    if os.getenv("CHROME_BIN"):
        chrome_options.binary_location = os.getenv("CHROME_BIN")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=chrome_options)

# -------------------- Scrape helpers --------------------
def set_page_length_100(driver):
    """Ø§Ù†Ø¯Ø§Ø²Ù‡ ØµÙØ­Ù‡ DataTables Ø±Ø§ ØªØ§ 100 Ø±Ø¯ÛŒÙ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ¨Ø±Ø¯ (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)."""
    try:
        length_sel = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.NAME, f"{TABLE_ID}_length"))
        )
        Select(length_sel).select_by_value("100")
        # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø±Ù†Ø¯Ø± Ù…Ø¬Ø¯Ø¯
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, f"#{TABLE_ID} tbody tr"))
        )
        time.sleep(0.3)
    except Exception:
        # Ø§Ú¯Ø± Ø³ÙÙ„ÙÚ©ØªÙˆØ± Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (Û³Û° ØªØ§ÛŒÛŒ) Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        pass

def parse_current_page(driver):
    """Ø®Ø±ÙˆØ¬ÛŒ: list[tuple(date, open, high, low, close)] Ø§Ø² Ù‡Ù…ÛŒÙ† ØµÙØ­Ù‡"""
    soup = BeautifulSoup(driver.page_source, "html.parser")
    table = soup.find("table", {"id": TABLE_ID})
    tbody = table.find("tbody")
    out = []
    for tr in tbody.find_all("tr"):
        tds = tr.find_all("td")
        if len(tds) == 8:
            open_, low, high, close, *_ , date_gregorian, _ = [
                td.text.strip().replace(",", "") for td in tds
            ]
            out.append((date_gregorian, open_, high, low, close))
    return out

def safe_click_next(driver):
    """Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡Ù” Ø¨Ø¹Ø¯ÛŒ Ú©Ù„ÛŒÚ© Ù…ÛŒâ€ŒÚ©Ù†Ø¯Ø› Ø§Ú¯Ø± Ø¹Ù†ØµØ± Ø¯ÛŒÚ¯Ø±ÛŒ Ú©Ù„ÛŒÚ© Ø±Ø§ Ú¯Ø±ÙØªØŒ Ø¨Ø§ JS Ú©Ù„ÛŒÚ© Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    next_btn = driver.find_element(By.ID, f"{TABLE_ID}_next")
    if "disabled" in next_btn.get_attribute("class"):
        return False

    # Ø§Ø³Ú©Ø±ÙˆÙ„ ØªØ§ Ø¯Ú©Ù…Ù‡ Ø¯Ø± Ø¯ÛŒØ¯ Ø¨Ø§Ø´Ø¯
    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", next_btn)
    time.sleep(0.1)

    try:
        WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.ID, f"{TABLE_ID}_next"))
        )
        next_btn.click()
    except (ElementClickInterceptedException, TimeoutException):
        # Ø§Ú¯Ø± Ú†ÛŒØ²ÛŒ Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡ Ø§ÙØªØ§Ø¯ØŒ Ø¨Ø§ JS Ú©Ù„ÛŒÚ© Ú©Ù†
        driver.execute_script("arguments[0].click();", next_btn)
    return True

def scrape_all_pages():
    driver = build_driver()
    driver.get(URL)

    # ØµØ¨Ø± ØªØ§ Ø¬Ø¯ÙˆÙ„ Ø¨ÛŒØ§ÛŒØ¯
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, TABLE_ID)))
    # Ø§Ù†Ø¯Ø§Ø²Ù‡ ØµÙØ­Ù‡ Ø±Ø§ Ø²ÛŒØ§Ø¯ Ú©Ù† ØªØ§ ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ú©Ù… Ø´ÙˆØ¯
    set_page_length_100(driver)

    all_rows = []

    # Ø®ÙˆØ§Ù†Ø¯Ù† Ø§ÙˆÙ„ÛŒÙ† ØµÙØ­Ù‡
    all_rows.extend(parse_current_page(driver))

    while True:
        # Ù†Ø´Ø§Ù†Ù‡Ù” ØªØºÛŒÛŒØ± ØµÙØ­Ù‡: Ù…ØªÙ† Ø§ÙˆÙ„ÛŒÙ† Ø±Ø¯ÛŒÙ
        try:
            first_row_el = driver.find_element(By.CSS_SELECTOR, f"#{TABLE_ID} tbody tr:first-child")
            first_text = first_row_el.text
        except Exception:
            first_text = None

        progressed = safe_click_next(driver)
        if not progressed:
            break

        # ØµØ¨Ø± ØªØ§ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ ØªØºÛŒÛŒØ± Ú©Ù†Ø¯ (ØµÙØ­Ù‡ Ø¹ÙˆØ¶ Ø´ÙˆØ¯)
        try:
            WebDriverWait(driver, 20).until(
                lambda d: d.find_element(By.CSS_SELECTOR, f"#{TABLE_ID} tbody tr:first-child").text != first_text
            )
        except TimeoutException:
            time.sleep(0.5)

        time.sleep(0.2)
        all_rows.extend(parse_current_page(driver))

    driver.quit()
    return all_rows

# -------------------- DataFrame & DB --------------------
def to_dataframe(rows):
    df = pd.DataFrame(rows, columns=["date_miladi", "open", "high", "low", "close"])
    df["date_miladi"] = pd.to_datetime(df["date_miladi"], format="%Y/%m/%d")
    for c in ["open", "high", "low", "close"]:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    df = df.dropna(subset=["date_miladi", "open", "high", "low", "close"])
    df = df.drop_duplicates(subset=["date_miladi"]).sort_values("date_miladi")
    return df

def upsert_to_db(df):
    conn = psycopg2.connect(os.getenv("DB_URL_SYNC"))
    cur = conn.cursor()

    # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: ÛŒÚ©â€ŒØ¨Ø§Ø± Ø¯Ø± DB Ø§Ø¬Ø±Ø§ Ú©Ù† ØªØ§ Ø¢Ù¾Ø³Ø±Øª Ú©Ø§Ø± Ú©Ù†Ø¯:
    # ALTER TABLE dollar_data ADD CONSTRAINT uq_dollar_date UNIQUE (date_miladi);

    upsert_sql = """
        INSERT INTO dollar_data (date_miladi, open, high, low, close)
        VALUES (%s, %s, %s, %s, %s)
        ON CONFLICT (date_miladi) DO UPDATE
        SET open = EXCLUDED.open,
            high = EXCLUDED.high,
            low  = EXCLUDED.low,
            close= EXCLUDED.close;
    """
    cur.executemany(upsert_sql, list(df.itertuples(index=False, name=None)))
    conn.commit()
    cur.close()
    conn.close()
    print(f"âœ… {len(df)} Ø±Ø¯ÛŒÙ Ø¯Ø±Ø¬/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")

# -------------------- Main --------------------
if __name__ == "__main__":
    rows = scrape_all_pages()
    print(f"ğŸ“¦ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ: {len(rows)} Ø±Ø¯ÛŒÙ (Ù‡Ù…Ù‡Ù” ØµÙØ­Ø§Øª).")
    df = to_dataframe(rows)
    upsert_to_db(df)
