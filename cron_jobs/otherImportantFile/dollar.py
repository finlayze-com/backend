# #Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ÙˆÙ‚ØªÛŒ Ú©Ù‡ Ú©Ù„ Ø¯ÛŒØªØ§ Ø¨ÛŒØ³ ØªØ§Ø±ÛŒØ®ÛŒ Ø±Ùˆ Ù…ÛŒØ®ÙˆØ§ÛŒÙ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒÙ… ØªÙˆ Ø¯ÛŒØªØ§ Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†ÛŒÙ…
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
# from webdriver_manager.chrome import ChromeDriverManager
# from selenium.webdriver.chrome.options import Options
# from bs4 import BeautifulSoup
# import pandas as pd
# import psycopg2
# import time
#
# # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø±ÙˆØ±Ú¯Ø± (headless)
# options = Options()
# options.add_argument("--headless")
# options.add_argument("--no-sandbox")
# options.add_argument("--disable-dev-shm-usage")
#
# # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
#
# # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª
# url = 'https://www.tgju.org/profile/price_dollar_rl/history'
# driver.get(url)
# time.sleep(5)
#
# # Ù¾Ø§Ø±Ø³ HTML Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ø¬Ø§ÙˆØ§Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
# soup = BeautifulSoup(driver.page_source, 'html.parser')
# driver.quit()
#
# # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„
# table = soup.find('table', {'id': 'DataTables_Table_0'})
# tbody = table.find('tbody')
#
# # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# rows = []
# for tr in tbody.find_all('tr'):
#     tds = tr.find_all('td')
#     if len(tds) == 8:
#         open_, low, high, close, *_ , date_gregorian, _ = [td.text.strip().replace(',', '') for td in tds]
#         rows.append((date_gregorian, open_, high, low, close))
#
# # Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
# df = pd.DataFrame(rows, columns=['date_miladi', 'open', 'high', 'low', 'close'])
# df['date_miladi'] = pd.to_datetime(df['date_miladi'], format='%Y/%m/%d')
#
# # Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
# conn = psycopg2.connect(
#     host='localhost',
#     dbname='postgres1',
#     user='postgres',
#     password='Afiroozi12'
# )
# cur = conn.cursor()
#
# # Ú¯Ø±ÙØªÙ† ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‡Ø³ØªÙ†Ø¯
# cur.execute("SELECT date_miladi FROM dollar_data;")
# existing_dates = set(row[0] for row in cur.fetchall())
#
# # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ§Ø±ÛŒØ®Ø´Ø§Ù† Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª
# df_new = df[~df['date_miladi'].isin(existing_dates)]
#
# # Ø¯Ø±Ø¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
# insert_query = """
#     INSERT INTO dollar_data (date_miladi, open, high, low, close)
#     VALUES (%s, %s, %s, %s, %s);
# """
# records = list(df_new.itertuples(index=False, name=None))
# if records:
#     cur.executemany(insert_query, records)
#     conn.commit()
#
# print(f"âœ… {len(records)} Ø±Ø¯ÛŒÙ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ dollar_data Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
#
# cur.close()
# conn.close()


# -*- coding: utf-8 -*-
"""
Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø§Ø² tgju Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Postgres.

- Ø¨Ø¯ÙˆÙ† webdriver-manager (Ø­Ù„ Ù…Ø´Ú©Ù„ CDN)
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² chromedriver Ø³ÛŒØ³ØªÙ…
- Ø®ÙˆØ§Ù†Ø¯Ù† DB_URL Ø§Ø² .env (fallback Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø«Ø§Ø¨Øª)
- Ø¢Ù¾Ø³Ø±Øª Ø±ÙˆÛŒ date_miladi

Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:
    source .venv/bin/activate
    python -m cron_jobs.otherImportantFile.dollar
"""
import os
import sys
import time
import shutil
import traceback
from pathlib import Path
from datetime import datetime

from dotenv import load_dotenv
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC

import psycopg2
from psycopg2.extras import execute_batch


# ---------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ ----------
REPO_ROOT = Path(__file__).resolve().parents[2]  # /root/backend
ENV_PATH = REPO_ROOT / ".env"

URL = "https://www.tgju.org/profile/price_dollar_rl/history"

HEADLESS = True
PAGELOAD_WAIT = 6   # seconds
WAIT_TIMEOUT = 20   # WebDriverWait seconds


# ---------- DB helpers ----------
def normalize_psycopg2_url(url: str) -> str:
    """psycopg2 ÙÙ‚Ø· postgresql:// Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ø¯."""
    return (
        url.replace("postgresql+asyncpg://", "postgresql://")
           .replace("postgresql+psycopg2://", "postgresql://")
    )

def get_db_url():
    # Ø§ÙˆÙ„ÙˆÛŒØª: sync â†’ async
    db_url = os.getenv("DB_URL_SYNC") or os.getenv("DB_URL") or os.getenv("DATABASE_URL")
    if not db_url:
        host = os.getenv("DB_HOST", "localhost")
        dbname = os.getenv("DB_NAME", "postgres1")
        user = os.getenv("DB_USER", "postgres")
        password = os.getenv("DB_PASSWORD", "Afiroozi12")
        port = os.getenv("DB_PORT", "5432")
        db_url = f"postgresql://{user}:{password}@{host}:{port}/{dbname}"
    return normalize_psycopg2_url(db_url)

def connect_db(db_url: str):
    conn = psycopg2.connect(db_url)
    conn.autocommit = False
    return conn


# ---------- Ø³Ø§ÛŒØ± Ú©Ù…Ú©â€ŒØªØ§Ø¨Ø¹â€ŒÙ‡Ø§ ----------
def log(msg: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)

def get_chromedriver_path() -> str:
    p = shutil.which("chromedriver")
    if p:
        return p
    for c in ["/usr/bin/chromedriver",
              "/usr/lib/chromium-browser/chromedriver",
              "/usr/lib/chromium/chromedriver",
              "/usr/local/bin/chromedriver"]:
        if os.path.exists(c):
            return c
    raise RuntimeError(
        "chromedriver ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†ØµØ¨ Ú©Ù†:\n"
        "  sudo apt-get install -y chromium-driver\n"
    )

def new_driver():
    options = Options()
    # Ø¨Ø¹Ø¶ÛŒ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ --headless=new Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ù†Ø¯Ø› ÙˆÙ„ÛŒ Ø±ÙˆÛŒ Ú©Ø±ÙˆÙ…ÛŒÙˆÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ØªØ±Ù‡
    if HEADLESS:
        try:
            options.add_argument("--headless=new")
        except Exception:
            options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1440,900")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-features=VizDisplayCompositor")
    options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/118.0.0.0 Safari/537.36")
    # options.binary_location = "/usr/bin/chromium-browser"  # Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²

    # Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± Ø§Ø² chromedriver Ø³ÛŒØ³ØªÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ù…Ù†ÙˆØ¹/Ø¨Ù„Ø§Ú© Ø§Ø³Øª)
    service = Service(executable_path=get_chromedriver_path())
    return webdriver.Chrome(service=service, options=options)

def parse_visible_rows_from_dom(html: str):
    """Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ ØµÙØ­Ù‡Ù” ÙØ¹Ù„ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø§Ø² DOM Ù¾Ø§Ø±Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", {"id": "DataTables_Table_0"})
    if not table:
        return []
    tbody = table.find("tbody")
    if not tbody:
        return []
    rows = []
    for tr in tbody.find_all("tr"):
        tds = [td.get_text(strip=True).replace(",", "") for td in tr.find_all("td")]
        if len(tds) < 7:
            continue

        # ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø§Ø² Ø±Ø§Ø³Øªâ€ŒØªØ±ÛŒÙ† Ø³ØªÙˆÙ† Ø´Ø¨ÛŒÙ‡ ØªØ§Ø±ÛŒØ® Ø¨Ø±Ø¯Ø§Ø±
        date_idx = None
        for i in range(len(tds)-1, -1, -1):
            token = tds[i]
            if "/" in token and len(token.split("/")) == 3:
                date_idx = i
                break
        if date_idx is None:
            date_idx = 6  # fallback

        date_gregorian = tds[date_idx]
        open_, low, high, close = tds[0:4]

        def to_float(x):
            try:
                return float(x)
            except:
                return None

        rows.append((
            date_gregorian.strip(),
            to_float(open_), to_float(high), to_float(low), to_float(close),
        ))
    return rows

def fetch_all_rows() -> list[tuple]:
    """Ù‡Ù…Ù‡ ØµÙØ­Ø§Øª DataTables Ø±Ø§ Ù¾ÛŒÙ…Ø§ÛŒØ´ Ùˆ Ù‡Ù…Ù‡Ù” Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
    driver = new_driver()
    try:
        driver.get(URL)
        wait = WebDriverWait(driver, WAIT_TIMEOUT)

        # ØµØ¨Ø± ØªØ§ Ø¬Ø¯ÙˆÙ„ Ù„ÙˆØ¯ Ø´ÙˆØ¯
        wait.until(EC.presence_of_element_located((By.ID, "DataTables_Table_0")))
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#DataTables_Table_0 tbody tr")))

        # Ø§ÙØ²Ø§ÛŒØ´ Ø·ÙˆÙ„ ØµÙØ­Ù‡ (Ø§Ú¯Ø± select ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
        try:
            length_select = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#DataTables_Table_0_length select")))
            try:
                Select(length_select).select_by_visible_text("100")
            except Exception:
                # Ø§Ú¯Ø± "100" Ù†Ø¨ÙˆØ¯ØŒ Ø¢Ø®Ø±ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡Ù” Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†
                try:
                    Select(length_select).select_by_index(len(Select(length_select).options)-1)
                except Exception:
                    pass
            # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø±Ù†Ø¯Ø± Ù…Ø¬Ø¯Ø¯
            time.sleep(1.0)
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#DataTables_Table_0 tbody tr")))
        except Exception:
            pass  # Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø¯Ø§Ù…Ù‡

        all_rows = []
        # ØµÙØ­Ù‡Ù” Ø§ÙˆÙ„
        all_rows.extend(parse_visible_rows_from_dom(driver.page_source))

        # Ù¾Ø§Ø¨Ù„ÛŒØ´Ø± DataTables: Ø¯Ú©Ù…Ù‡ Next Ø¨Ø§ id #DataTables_Table_0_next
        while True:
            try:
                next_btn = driver.find_element(By.CSS_SELECTOR, "#DataTables_Table_0_next")
            except Exception:
                break

            classes = (next_btn.get_attribute("class") or "")
            if "disabled" in classes:
                break

            # Ú©Ù„ÛŒÚ© Next Ùˆ ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ³Ø§Ø²ÛŒ Ø³Ø·Ø±Ù‡Ø§
            # Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù„ÛŒÚ©ØŒ ÛŒÚ© Ø¹Ù†ØµØ± Ø§Ø² Ø³Ø·Ø± ÙØ¹Ù„ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… ØªØ§ staleness Ú†Ú© Ø´ÙˆØ¯
            try:
                any_row = driver.find_element(By.CSS_SELECTOR, "#DataTables_Table_0 tbody tr")
            except Exception:
                any_row = None

            next_btn.click()

            if any_row:
                try:
                    wait.until(EC.staleness_of(any_row))
                except Exception:
                    pass

            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#DataTables_Table_0 tbody tr")))
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø·Ø±Ù‡Ø§ÛŒ ØµÙØ­Ù‡Ù” Ø¬Ø¯ÛŒØ¯
            all_rows.extend(parse_visible_rows_from_dom(driver.page_source))

        return all_rows

    finally:
        try:
            driver.quit()
        except Exception:
            pass

def parse_all_pages_to_df() -> pd.DataFrame:
    rows = fetch_all_rows()
    if not rows:
        raise ValueError("Ù‡ÛŒÚ† Ø³Ø·Ø±ÛŒ Ø§Ø² Ø¬Ø¯ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯.")
    df = pd.DataFrame(rows, columns=["date_miladi", "open", "high", "low", "close"])

    def parse_date(s):
        s = (s or "").strip()
        for fmt in ("%Y/%m/%d", "%Y-%m-%d"):
            try:
                return datetime.strptime(s, fmt).date()
            except:
                pass
        return None

    df["date_miladi"] = df["date_miladi"].map(parse_date)
    df = df.dropna(subset=["date_miladi"]).copy()
    # Ø­Ø°Ù Ø¯ÙˆØ¨Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
    df = df.drop_duplicates(subset=["date_miladi"], keep="first")
    return df

def ensure_table(conn):
    ddl = """
    CREATE TABLE IF NOT EXISTS public.dollar_data (
        date_miladi date PRIMARY KEY,
        open numeric,
        high numeric,
        low  numeric,
        close numeric
    );
    """
    with conn.cursor() as cur:
        cur.execute(ddl)
    conn.commit()

def upsert_dollar_data(conn, df: pd.DataFrame):
    if df.empty:
        log("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return 0
    sql = """
    INSERT INTO public.dollar_data (date_miladi, open, high, low, close)
    VALUES (%s, %s, %s, %s, %s)
    ON CONFLICT (date_miladi)
    DO UPDATE SET
        open = EXCLUDED.open,
        high = EXCLUDED.high,
        low  = EXCLUDED.low,
        close= EXCLUDED.close;
    """
    records = [(r.date_miladi, r.open, r.high, r.low, r.close) for r in df.itertuples(index=False)]
    with conn.cursor() as cur:
        execute_batch(cur, sql, records, page_size=1000)
    conn.commit()
    return len(records)

def main():
    try:
        if ENV_PATH.exists():
            load_dotenv(ENV_PATH)
            log(f".env loaded from {ENV_PATH}")
        else:
            log("âš ï¸ .env Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø§Ø² env ÙØ¹Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù….")

        print("RAW DB_URL     =", repr(os.getenv("DB_URL")))
        print("RAW DB_URL_SYNC=", repr(os.getenv("DB_URL_SYNC")))

        db_url = get_db_url()
        log(f"EFFECTIVE DB_URL (psycopg2) = {db_url}")

        log("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ù…Ù‡ ØµÙØ­Ø§Øªâ€¦")
        df = parse_all_pages_to_df()
        log(f"ğŸ§® Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {len(df)}")

        conn = connect_db(db_url)
        try:
            ensure_table(conn)
            inserted = upsert_dollar_data(conn, df)
            log(f"âœ… {inserted} Ø±Ø¯ÛŒÙ Ø¯Ø±Ø¬/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯ Ø¯Ø± dollar_data.")
        finally:
            conn.close()

        log("ğŸ‰ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    except Exception:
        log("âŒ Ø®Ø·Ø§ Ø±Ø® Ø¯Ø§Ø¯:")
        print(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()