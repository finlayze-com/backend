import psycopg2
import pandas as pd
import logging
from datetime import timedelta
import jdatetime
from sqlalchemy import create_engine, text
from finpy_tse import Get_Queue_History
from dotenv import load_dotenv
import os
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy import MetaData, Table

# ---------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ ---------------------- #
logging.basicConfig(
    filename='../../queue_fetch.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# ---------------------- ØªØ§Ø¨Ø¹ ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø´Ù…Ø³ÛŒ ---------------------- #
def to_jalali_str(greg_date):
    return jdatetime.date.fromgregorian(date=greg_date).strftime('%Y-%m-%d')


# ---------------------- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² .env ---------------------- #
load_dotenv()  # ÙØ§ÛŒÙ„ .env Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯

DB_URL_SYNC = os.getenv("DB_URL_SYNC") or os.getenv("DB_URL")
if not DB_URL_SYNC:
    raise EnvironmentError("âŒ Ù…ØªØºÛŒØ± DB_URL ÛŒØ§ DB_URL_SYNC Ø¯Ø± ÙØ§ÛŒÙ„ .env ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ try/except
try:
    conn = psycopg2.connect(DB_URL_SYNC)
    cursor = conn.cursor()
    logging.info("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯.")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
    raise


# ---------------------- ØªØ¹ÛŒÛŒÙ† Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø§Ø² daily_stock_data ---------------------- #
try:
    cursor.execute('SELECT MAX("Timestamp"::date) FROM orderbook_snapshot;')
    last_trading_date = cursor.fetchone()[0]
    if last_trading_date is None:
        raise Exception("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ orderbook_snapshot ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    start_date = to_jalali_str(last_trading_date- timedelta(days=1))
    end_date = to_jalali_str(last_trading_date )
    logging.info(f"ğŸ“… Ø¯Ø±ÛŒØ§ÙØª ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®: {start_date}")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ: {e}")
    conn.close()
    raise

# ---------------------- Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ ---------------------- #
try:
    cursor.execute(
        '''
                SELECT DISTINCT "stock_ticker"
                FROM symboldetail
                WHERE "stock_ticker" IS NOT NULL
                  AND instrument_type = 'saham'
                ORDER BY "stock_ticker"
                '''
        )
    rows = cursor.fetchall()
    tickers = [row[0] for row in rows]
    logging.info(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø³Ù‡Ø§Ù… (instrument_type='saham'): {len(tickers)}")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
    conn.close()
    raise

# ---------------------- Ø¯Ø±ÛŒØ§ÙØª ØµÙâ€ŒÙ‡Ø§ ---------------------- #
all_data = []
for ticker in tickers:
    try:
        data = Get_Queue_History(ticker, start_date, end_date)

        if data is None:
            logging.warning(f"{ticker} - âš ï¸ Ø®Ø±ÙˆØ¬ÛŒ None Ø¨ÙˆØ¯.")
            continue

        if isinstance(data, pd.DataFrame):
            df = data.copy()
        elif isinstance(data, dict):
            df = pd.DataFrame([data])
        else:
            logging.warning(f"{ticker} - âš ï¸ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø§Ø´Ù†Ø§Ø³: {type(data)}")
            continue

        df['stock_ticker'] = ticker
        df['date'] = start_date

        # Ø­Ø°Ù ØµÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ BQ Ùˆ SQ Ù‡Ø± Ø¯Ùˆ ØµÙØ± Ù‡Ø³ØªÙ†Ø¯
        df = df[(df['BQ_Value'] != 0) | (df['SQ_Value'] != 0)]

        if not df.empty:
            all_data.append(df)
        else:
            logging.info(f"{ticker} - ØµÙâ€ŒÙ‡Ø§ ØµÙØ± Ø¨ÙˆØ¯Ù† Ùˆ Ø­Ø°Ù Ø´Ø¯.")

    except Exception as e:
        logging.error(f"{ticker} - âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡: {e}")

# ---------------------- Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ + Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯ ---------------------- #
## ---------------------- UPSERT ---------------------- #
if all_data:
    final_df = pd.concat(all_data, ignore_index=True).sort_values(by='BQ_Value', ascending=False)
    final_df = final_df.where(pd.notnull(final_df), None)  # NaN -> None

    logging.info(f"ğŸ“Š {len(final_df)} Ø±Ø¯ÛŒÙ Ø¨Ø±Ø§ÛŒ UPSERT Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.")

    try:
        engine = create_engine(DB_URL_SYNC)
        conflict_cols = ["stock_ticker", "date"]  # Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§
        records = final_df.to_dict(orient="records")

        with engine.begin() as connection:
            md = MetaData()
            quote = Table("quote", md, autoload_with=connection)  # reflect Ø¬Ø¯ÙˆÙ„

            update_cols = [c.name for c in quote.columns if c.name not in conflict_cols]

            chunk_size = 1000
            for i in range(0, len(records), chunk_size):
                chunk = records[i:i+chunk_size]

                stmt = pg_insert(quote).values(chunk)
                update_map = {c: getattr(stmt.excluded, c) for c in update_cols}

                upsert_stmt = stmt.on_conflict_do_update(
                    index_elements=conflict_cols,
                    set_=update_map
                )
                connection.execute(upsert_stmt)

        logging.info("âœ… UPSERT Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± UPSERT Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
else:
    logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª.")

# ---------------------- Ø¨Ø³ØªÙ† Ø§ØªØµØ§Ù„ ---------------------- #
cursor.close()
conn.close()
