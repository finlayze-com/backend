# -*- coding: utf-8 -*-
"""
Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Â«ÙÙ‚Ø· ØµÙâ€ŒØ¯Ø§Ø±Ù‡Ø§Â» (ØµÙ Ø®Ø±ÛŒØ¯ ÛŒØ§ ØµÙ ÙØ±ÙˆØ´) Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² Â«Ø¯ÛŒØ±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒÂ»
- Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² finpy_tse
- Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ù‚Ø¨Ù„ÛŒ (quote) Ø¨Ø§ Ú©Ù„ÛŒØ¯ (inscode, date)
- Ø´Ø§Ù…Ù„ Value Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø² InstTradeHistory
- Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† base_value = adjust_high * baseVol
"""

import os
import logging
from datetime import datetime, timezone

import requests
import pandas as pd
import psycopg2
import jdatetime
from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert
from dotenv import load_dotenv

# ---------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ ---------------------- #
HEADERS = {"User-Agent": "Mozilla/5.0"}
TIMEOUT = 30  # Ø«Ø§Ù†ÛŒÙ‡
CLOSE_HEVEN = 123000  # 12:30:00 â†’ 123000 (HHMMSS)
CHUNK_SIZE = 1000

logging.basicConfig(
    filename='../../queue_fetch.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# --- Ú©Ù…Ú©ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® --- #
def to_jalali_str(greg_date):
    """ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒÙ date Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ø¬Ù„Ø§Ù„ÛŒ YYYY-MM-DD"""
    return jdatetime.date.fromgregorian(date=greg_date).strftime('%Y-%m-%d')


def j2g_yyyymmdd(jdate_str: str) -> str:
    """ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ Ø¬Ù„Ø§Ù„ÛŒ YYYY-MM-DD Ø¨Ù‡ Ù…ÛŒÙ„Ø§Ø¯ÛŒ ÙØ´Ø±Ø¯Ù‡ YYYYMMDD (Ø¨Ø±Ø§ÛŒ API)"""
    y, m, d = map(int, jdate_str.split('-'))
    g = jdatetime.date(y, m, d).togregorian()
    return f"{g.year:04}{g.month:02}{g.day:02}"


def h_even_to_timestr(h: int) -> str:
    """ØªØ¨Ø¯ÛŒÙ„ hEven Ù…Ø«Ù„ 123000 Ø¨Ù‡ HH:MM:SS"""
    s = str(int(h)).zfill(6)
    return f"{s[:2]}:{s[2:4]}:{s[4:]}"


# ---------------------- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ .env ---------------------- #
load_dotenv()
DB_URL_SYNC = os.getenv("DB_URL_SYNC") or os.getenv("DB_URL")
if not DB_URL_SYNC:
    raise EnvironmentError("âŒ Ù…ØªØºÛŒØ± DB_URL ÛŒØ§ DB_URL_SYNC Ø¯Ø± ÙØ§ÛŒÙ„ .env ÛŒØ§ÙØª Ù†Ø´Ø¯.")

print("ğŸ”¹ Start: connecting to DB ...")
conn = psycopg2.connect(DB_URL_SYNC)
cursor = conn.cursor()
logging.info("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯.")
print("   âœ… DB connected")

# ---------------------- ØªØ¹ÛŒÛŒÙ† ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù ---------------------- #
try:
    print("ğŸ”¹ Resolving target date from orderbook_snapshot ...")
    cursor.execute('SELECT MAX("Timestamp"::date) FROM orderbook_snapshot;')
    last_trading_date = cursor.fetchone()[0]
    if last_trading_date is None:
        raise RuntimeError("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ orderbook_snapshot ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

    date_g = last_trading_date              # date Ù…ÛŒÙ„Ø§Ø¯ÛŒ
    date_j = to_jalali_str(date_g)          # Ø¬Ù„Ø§Ù„ÛŒ YYYY-MM-DD
    date_g_compact = j2g_yyyymmdd(date_j)   # Ù…ÛŒÙ„Ø§Ø¯ÛŒ ÙØ´Ø±Ø¯Ù‡ YYYYMMDD Ø¨Ø±Ø§ÛŒ API

    msg = f"ğŸ“… Target Date â†’ Gregorian={date_g} | Jalali={date_j} | Compact={date_g_compact}"
    logging.info(msg)
    print("   âœ…", msg)
except Exception as e:
    logging.exception("âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¹ÛŒÛŒÙ† ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù: %s", e)
    print("   âŒ Error determining target date:", e)
    cursor.close()
    conn.close()
    raise

# ---------------------- Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ùˆ inscode ---------------------- #
try:
    print("ğŸ”¹ Fetching saham tickers & insCodes from symboldetail ...")
    cursor.execute("""
        SELECT DISTINCT "stock_ticker", "insCode"
        FROM symboldetail
        WHERE instrument_type = 'saham'
          AND "stock_ticker" IS NOT NULL
          AND "insCode" IS NOT NULL
        ORDER BY "stock_ticker";
    """)
    tickers = [(r[0], str(r[1])) for r in cursor.fetchall()]
    logging.info(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø³Ù‡Ø§Ù…: {len(tickers)}")
    print(f"   âœ… Found {len(tickers)} saham tickers")
except Exception as e:
    logging.exception("âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§: %s", e)
    print("   âŒ Error fetching tickers:", e)
    cursor.close()
    conn.close()
    raise


# ---------------------- ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ APIÙ‡Ø§ÛŒ TSETMC ---------------------- #
def get_thresholds(inscode: str, yyyymmdd: str):
    """
    Ø³Ù‚Ù/Ú©Ù Ø¯Ø§Ù…Ù†Ù‡ Ø±ÙˆØ² Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ® Ù‡Ø¯Ù Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
    Ø®Ø±ÙˆØ¬ÛŒ: (day_ub, day_ll) Ø¨Ù‡ int
    """
    url = f"https://cdn.tsetmc.com/api/MarketData/GetStaticThreshold/{inscode}/{yyyymmdd}"
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    js = r.json()
    df = pd.DataFrame(js.get("staticThreshold", []))
    if df.empty:
        raise RuntimeError("Empty staticThreshold")
    row = df.iloc[-1]
    day_ub = int(row["psGelStaMax"])
    day_ll = int(row["psGelStaMin"])
    return day_ub, day_ll


def get_bestlimits_snapshot(inscode: str, yyyymmdd: str):
    """
    Ø§Ø² BestLimits Ø¢Ø±Ø´ÛŒÙˆÛŒ Ù‡Ù…Ø§Ù† Ø±ÙˆØ²ØŒ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ø§Ø³Ù†Ù¾â€ŒØ´Ø§Øª Ø¨Ù‡ 12:30:00 Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø§Ø³Ù†Ù¾â€ŒØ´Ø§ØªÛŒ â‰¤ 12:30 Ù†Ø¨ÙˆØ¯ØŒ Ø¢Ø®Ø±ÛŒÙ† Ø§Ø³Ù†Ù¾â€ŒØ´Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ø§Ù† Ø±ÙˆØ² Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ø®Ø±ÙˆØ¬ÛŒ: dict ÛŒÚ© Ø±Ø¯ÛŒÙ (top level) ÛŒØ§ None
    """
    url = f"https://cdn.tsetmc.com/api/BestLimits/{inscode}/{yyyymmdd}"
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    js = r.json()

    # Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø§Ø³Ø® Ù…Ù…Ú©Ù† Ø§Ø³Øª dict ÛŒØ§ Ù„ÛŒØ³Øª Ø¨Ø§Ø´Ø¯
    if isinstance(js, dict):
        rows = js.get("bestLimitsHistory", js.get("bestLimits", []))
    else:
        rows = js

    df = pd.DataFrame(rows)
    if df.empty:
        return None

    # hEven Ø¨Ø§ÛŒØ¯ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ø¯
    df = df[pd.to_numeric(df["hEven"], errors="coerce").notnull()]
    df["hEven"] = df["hEven"].astype(int)

    # Ø§ÙˆÙ„ÙˆÛŒØª: Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† hEven â‰¤ 123000
    sub = df[df["hEven"] <= CLOSE_HEVEN]
    if not sub.empty:
        tmax = sub["hEven"].max()
        snap = sub[sub["hEven"] == tmax].sort_values("number").head(1).iloc[0].to_dict()
        return snap

    # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø¢Ø®Ø±ÛŒÙ† hEven Ø±ÙˆØ²
    tmax_all = df["hEven"].max()
    snap = df[df["hEven"] == tmax_all].sort_values("number").head(1).iloc[0].to_dict()
    return snap


def get_value_from_old_endpoint(inscode: str, yyyymmdd: str):
    """
    Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±ÙˆØ²Ø§Ù†Ù‡ (Value) Ø±Ø§ Ø§Ø² endpoint Ù‚Ø¯ÛŒÙ…ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ ÙÙ‚Ø· Ù‡Ù…Ø§Ù† ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ â†’ 0
    """
    url = f"https://old.tsetmc.com/tsev2/data/InstTradeHistory.aspx?i={inscode}&Top=999999&A=0"
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    txt = r.text.strip()
    if not txt:
        return 0

    # columns=['Date','High','Low','Final','Close','Open','Y-Final','Value','Volume','No']
    last_value = 0
    for row in txt.split(";"):
        parts = row.split("@")
        if len(parts) < 9:
            continue
        d = parts[0]
        if d == yyyymmdd:
            try:
                last_value = int(float(parts[7]))
            except Exception:
                last_value = 0
            break
    return last_value


def compute_queues_from_snapshot(snap: dict, day_ub: int, day_ll: int):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ùˆ Ø³Ø±Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ snapshot Ø®Ø±ÙˆØ¬ÛŒ BestLimits.
    - ØµÙ Ø®Ø±ÛŒØ¯: Ù‚ÛŒÙ…Øª Ø®Ø±ÛŒØ¯ Ø³Ø·Ø­ 1 == Ø³Ù‚Ù Ø±ÙˆØ²
    - ØµÙ ÙØ±ÙˆØ´: Ù‚ÛŒÙ…Øª ÙØ±ÙˆØ´ Ø³Ø·Ø­ 1 == Ú©Ù Ø±ÙˆØ²
    """
    p_buy = snap.get("pMeDem", snap.get("Price_Buy"))
    q_buy = snap.get("qTitMeDem", snap.get("Vol_Buy"))
    n_buy = snap.get("zOrdMeDem", snap.get("No_Buy"))

    p_sell = snap.get("pMeOf", snap.get("Price_Sell"))
    q_sell = snap.get("qTitMeOf", snap.get("Vol_Sell"))
    n_sell = snap.get("zOrdMeOf", snap.get("No_Sell"))

    p_buy = float(p_buy) if p_buy is not None else 0.0
    q_buy = int(q_buy) if q_buy is not None else 0
    n_buy = int(n_buy) if n_buy is not None else 0

    p_sell = float(p_sell) if p_sell is not None else 0.0
    q_sell = int(q_sell) if q_sell is not None else 0
    n_sell = int(n_sell) if n_sell is not None else 0

    bq_value = 0
    sq_value = 0
    bqpc = 0
    sqpc = 0

    # ØµÙ ÙØ±ÙˆØ´
    if p_sell == float(day_ll):
        sq_value = int(day_ll * q_sell)
        sqpc = int(sq_value // max(n_sell, 1))

    # ØµÙ Ø®Ø±ÛŒØ¯
    if p_buy == float(day_ub):
        bq_value = int(day_ub * q_buy)
        bqpc = int(bq_value // max(n_buy, 1))

    time_close = h_even_to_timestr(int(snap.get("hEven", CLOSE_HEVEN)))
    return bq_value, sq_value, bqpc, sqpc, time_close


# ---------------------- Ø¯Ø±ÛŒØ§ÙØª adjust_high Ùˆ baseVol Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø§Ù† Ø±ÙˆØ² ---------------------- #
def get_base_parts(inscode: str, yyyymmdd: str):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ adjust_high Ùˆ baseVol Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø§Ù† ØªØ§Ø±ÛŒØ®:
      - adjust_high Ø§Ø² InstTradeHistory (A=1 â†’ ØªØ¹Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡)
      - baseVol Ø§Ø² GetInstrumentInfo (Ø¬Ø¯ÛŒØ¯)
    Ø®Ø±ÙˆØ¬ÛŒ: (adjust_high, base_vol)
    """
    adjust_high = None
    base_vol = None

    # --- Adjusted High ---
    try:
        url = f"https://old.tsetmc.com/tsev2/data/InstTradeHistory.aspx?i={inscode}&Top=999999&A=1"
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        r.raise_for_status()
        txt = r.text.strip()
        for row in txt.split(";"):
            parts = row.split("@")
            if len(parts) < 2:
                continue
            if parts[0] == yyyymmdd:
                adjust_high = float(parts[1])  # Ø³ØªÙˆÙ† High
                break
    except Exception as e:
        logging.warning(f"{inscode} - AdjustHigh fail: {e}")

    # --- BaseVol ---
    try:
        url = f"https://cdn.tsetmc.com/api/Instrument/GetInstrumentInfo/{inscode}"
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        if r.ok:
            js = r.json()
            info = js.get("instrumentInfo", {})
            base_vol = int(float(info.get("baseVol", 0) or 0))
    except Exception as e:
        logging.warning(f"{inscode} - BaseVol fail: {e}")

    return adjust_high, base_vol


# ---------------------- Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÙ‚Ø· ØµÙâ€ŒØ¯Ø§Ø±Ù‡Ø§ ---------------------- #
engine = create_engine(DB_URL_SYNC)
records = []
downloaded_at = datetime.now(timezone.utc)

print(f"ğŸ”¸ Processing {len(tickers)} tickers for date {date_g} (J:{date_j}) ...")

for idx, (stock_ticker, ins) in enumerate(tickers, start=1):
    if idx % 50 == 1 or idx == len(tickers):
        print(f"   â€¦ {idx}/{len(tickers)}")

    # Ù„Ø§Ú¯ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ù„Ø§Ù† Ú©Ø¯Ø§Ù… Ù†Ù…Ø§Ø¯ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Øª
    print(f"â†’ Processing: {stock_ticker} ({ins})")
    logging.info(f"Processing: {stock_ticker} ({ins})")

    # --- Ù…Ø±Ø­Ù„Ù‡ Û±: Threshold ---
    try:
        day_ub, day_ll = get_thresholds(ins, date_g_compact)
    except Exception as e:
        logging.warning(f"{stock_ticker} ({ins}) - Threshold error: {e}")
        print(f"âŒ Threshold error for {stock_ticker} ({ins}): {e}")
        continue

    # --- Ù…Ø±Ø­Ù„Ù‡ Û²: BestLimits snapshot ---
    try:
        snap = get_bestlimits_snapshot(ins, date_g_compact)
    except Exception as e:
        logging.warning(f"{stock_ticker} ({ins}) - BestLimits error: {e}")
        print(f"âŒ BestLimits error for {stock_ticker} ({ins}): {e}")
        continue

    if not snap:
        logging.info(f"{stock_ticker} ({ins}) - No BestLimits snapshot, skipping.")
        print(f"âš ï¸ No BestLimits snapshot for {stock_ticker} ({ins}), skipping.")
        continue

    # --- Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµÙ ---
    try:
        bq_value, sq_value, bqpc, sqpc, time_close = compute_queues_from_snapshot(
            snap, day_ub, day_ll
        )

        # Ø¯ÛŒØ¨Ø§Ú¯ ÙˆÛŒÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆØ³Ù¾Ù‡
        if stock_ticker == 'ÙˆØ³Ù¾Ù‡' or ins == '2328862017676109':
            print(f"   [DEBUG ÙˆØ³Ù¾Ù‡] day_ub={day_ub}, day_ll={day_ll}")
            print(
                f"   [DEBUG ÙˆØ³Ù¾Ù‡] bq_value={bq_value}, sq_value={sq_value}, "
                f"bqpc={bqpc}, sqpc={sqpc}, time_close={time_close}"
            )
            print(
                "   [DEBUG ÙˆØ³Ù¾Ù‡] snap fields: "
                f"pMeDem={snap.get('pMeDem')}, qTitMeDem={snap.get('qTitMeDem')}, "
                f"pMeOf={snap.get('pMeOf')}, qTitMeOf={snap.get('qTitMeOf')}, "
                f"hEven={snap.get('hEven')}"
            )
    except Exception as e:
        logging.warning(f"{stock_ticker} ({ins}) - Queue compute error: {e}")
        print(f"âŒ Queue compute error for {stock_ticker} ({ins}): {e}")
        continue

    # ÙÙ‚Ø· Ø§Ú¯Ø± ØµÙ Ø®Ø±ÛŒØ¯ ÛŒØ§ ÙØ±ÙˆØ´ Ø¨Ø§Ø´Ø¯
    if bq_value <= 0 and sq_value <= 0:
        continue

    # --- Ù…Ø±Ø­Ù„Ù‡ Û´: Ø³Ø§ÛŒØ± Ø§Ø·Ù„Ø§Ø¹Ø§Øª (Value, base_value) Ùˆ Ø³Ø§Ø®Øª Ø±Ú©ÙˆØ±Ø¯ ---
    try:
        day_value = get_value_from_old_endpoint(ins, date_g_compact)  # Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±ÙˆØ²

        # Ú¯Ø±ÙØªÙ† adjust_high Ùˆ baseVol Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ base_value
        adj_high, base_vol = get_base_parts(ins, date_g_compact)
        if adj_high is not None and base_vol is not None:
            try:
                base_value = float(adj_high) * int(base_vol)
            except Exception:
                base_value = 0
        else:
            base_value = 0

        rec = {
            # Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§
            "inscode": ins,
            "date": to_jalali_str(date_g),

            # Ø³Ø§ÛŒØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
            "stock_ticker": stock_ticker,
            "downloaded_at": downloaded_at,
            "Day_UL": day_ub,
            "Day_LL": day_ll,
            "Time": time_close,
            "BQ_Value": bq_value,
            "SQ_Value": sq_value,
            "BQPC": bqpc,
            "SQPC": sqpc,
            "Value": day_value,       # Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±ÙˆØ²
            "base_value": base_value  # Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø¯ÛŒØ¯
        }
        records.append(rec)

    except Exception as e:
        logging.warning(f"{stock_ticker} ({ins}) - After-queue error: {e}")
        print(f"âŒ After-queue error for {stock_ticker} ({ins}): {e}")
        continue

if not records:
    logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø±Ú©ÙˆØ±Ø¯ ØµÙâ€ŒØ¯Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    print("\nğŸ”» No queue records to upsert.")
else:
    logging.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØµÙâ€ŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡: {len(records)}")
    print(f"\nğŸ”¹ Ready to UPSERT {len(records)} queued records into 'quote'")

    # --- UPSERT Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Â«quoteÂ» Ø¨Ø§ Ù‡Ù…Ø§Ù† Ú©Ù„ÛŒØ¯ (inscode, date) --- #
    try:
        with engine.begin() as connection:
            md = MetaData()
            quote = Table("quote", md, autoload_with=connection)

            table_cols = {c.name for c in quote.columns}
            filtered_records = [
                {k: v for k, v in rec.items() if k in table_cols}
                for rec in records
            ]

            if not filtered_records:
                logging.warning("âš ï¸ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ØŒ Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.")
                print("   âš ï¸ Nothing left after column-filtering (check table schema).")
            else:
                insert_stmt = pg_insert(quote).values(filtered_records)

                conflict_cols = ["inscode", "date"]
                update_cols = [c for c in table_cols if c not in conflict_cols]

                do_update = insert_stmt.on_conflict_do_update(
                    index_elements=conflict_cols,
                    set_={c: getattr(insert_stmt.excluded, c) for c in update_cols}
                )
                connection.execute(do_update)
                print("   âœ… UPSERT executed.")
        print("âœ… Done.")
    except Exception as e:
        print("   âŒ UPSERT error:", e)

cursor.close()
conn.close()
