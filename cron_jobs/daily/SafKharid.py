import psycopg2
import pandas as pd
import logging
import jdatetime
from sqlalchemy import create_engine, text
from finpy_tse import Get_Queue_History
from dotenv import load_dotenv
import os
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy import MetaData, Table
from datetime import timedelta, datetime, timezone

# ---------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ ---------------------- #
logging.basicConfig(
    filename='../../queue_fetch.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# ---------------------- ØªØ§Ø¨Ø¹ ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø´Ù…Ø³ÛŒ ---------------------- #
def to_jalali_str(greg_date):
    return jdatetime.date.fromgregorian(date=greg_date).strftime('%Y-%m-%d')


# ---------------------- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² .env ---------------------- #
load_dotenv()  # ÙØ§ÛŒÙ„ .env Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯

DB_URL_SYNC = os.getenv("DB_URL_SYNC") or os.getenv("DB_URL")
if not DB_URL_SYNC:
    raise EnvironmentError("âŒ Ù…ØªØºÛŒØ± DB_URL ÛŒØ§ DB_URL_SYNC Ø¯Ø± ÙØ§ÛŒÙ„ .env ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ try/except
try:
    conn = psycopg2.connect(DB_URL_SYNC)
    cursor = conn.cursor()
    logging.info("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯.")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
    raise


# ---------------------- ØªØ¹ÛŒÛŒÙ† Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø§Ø² daily_stock_data ---------------------- #
try:
    cursor.execute('SELECT MAX("Timestamp"::date) FROM orderbook_snapshot;')
    last_trading_date = cursor.fetchone()[0]
    if last_trading_date is None:
        raise Exception("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ orderbook_snapshot ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    start_date = to_jalali_str(last_trading_date- timedelta(days=0))
    end_date = to_jalali_str(last_trading_date )
    logging.info(f"ğŸ“… Ø¯Ø±ÛŒØ§ÙØª ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®: {start_date}")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ: {e}")
    conn.close()
    raise

# ---------------------- Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ ---------------------- #
try:
    cursor.execute(
        '''
                SELECT DISTINCT "stock_ticker"
                FROM symboldetail
                WHERE "stock_ticker" IS NOT NULL
                  AND instrument_type = 'saham'
                ORDER BY "stock_ticker"
                '''
        )
    rows = cursor.fetchall()
    tickers = [row[0] for row in rows]
    logging.info(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø³Ù‡Ø§Ù… (instrument_type='saham'): {len(tickers)}")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§: {e}")
    conn.close()
    raise

# ---------------------- Ù…Ù¾ ØªÛŒÚ©Ø± â†’ inscode ---------------------- #
try:
    cursor.execute(
        '''
        SELECT "stock_ticker", "insCode"
        FROM symboldetail
        WHERE instrument_type = 'saham'
          AND "stock_ticker" IS NOT NULL
          AND "insCode" IS NOT NULL
        '''
    )
    rows = cursor.fetchall()
    ticker2ins = {row[0]: str(row[1]) for row in rows}  # dict: ticker -> inscode
    logging.info(f"ğŸ§© Ù…Ù¾ ØªÛŒÚ©Ø±â†’inscode Ø¨Ø±Ø§ÛŒ {len(ticker2ins)} Ù†Ù…Ø§Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
except Exception as e:
    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† Ù…Ù¾ inscode: {e}")
    conn.close()
    raise

# ---------------------- Ø¯Ø±ÛŒØ§ÙØª ØµÙâ€ŒÙ‡Ø§ ---------------------- #
all_data = []
fetch_ts = datetime.now(timezone.utc)  # Ø²Ù…Ø§Ù† Ø«Ø¨Øª Ø¯Ø§Ù†Ù„ÙˆØ¯ (timezone-aware)
for ticker in tickers:
    fetch_ts = datetime.now(timezone.utc)  # Ø²Ù…Ø§Ù† Ø«Ø¨Øª Ø¯Ø§Ù†Ù„ÙˆØ¯ (timezone-aware)

    try:
        ins = ticker2ins.get(ticker)
        if not ins:
            logging.warning(f"{ticker} - inscode ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø±Ø¯ Ø´Ø¯.")
            continue

        data = Get_Queue_History(ticker, start_date, end_date)
        if data is None:
            logging.warning(f"{ticker} - âš ï¸ Ø®Ø±ÙˆØ¬ÛŒ None Ø¨ÙˆØ¯.")
            continue

        if isinstance(data, pd.DataFrame):
            df = data.copy()
        elif isinstance(data, dict):
            df = pd.DataFrame([data])
        else:
            logging.warning(f"{ticker} - âš ï¸ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø§Ø´Ù†Ø§Ø³: {type(data)}")
            continue

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        df['stock_ticker'] = ticker
        df['inscode'] = ins
        df['downloaded_at'] = fetch_ts  # ğŸ‘ˆ Ø²Ù…Ø§Ù† Ø¯Ø§Ù†Ù„ÙˆØ¯

        # âœ… ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø§Ø² J-Date (Ø³ØªÙˆÙ† ÛŒØ§ Ø§ÛŒÙ†Ø¯Ú©Ø³) Ø¨Ú¯ÛŒØ±Ø› Ø¯Ø± ØºÛŒØ±Ø§ÛŒÙ†ØµÙˆØ±Øª Ø§Ø² start_date
        if df.index.name and df.index.name.lower().replace('_','').replace('-','') in ('jdate','jdate'):
            df = df.reset_index().rename(columns={df.index.name: 'date'})
        elif 'J-Date' in df.columns:
            df = df.rename(columns={'J-Date': 'date'})
        else:
            df['date'] = start_date

        # ÙÙ‚Ø· Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ ØµÙØ± Ù†ÛŒØ³Øª
        if {'BQ_Value', 'SQ_Value'}.issubset(df.columns):
            df = df[(df['BQ_Value'] != 0) | (df['SQ_Value'] != 0)]
        else:
            logging.warning(f"{ticker} - Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ BQ_Value/SQ_Value Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.")
            continue

        if not df.empty:
            all_data.append(df)
        else:
            logging.info(f"{ticker} - ØµÙâ€ŒÙ‡Ø§ ØµÙØ± Ø¨ÙˆØ¯Ù† Ùˆ Ø­Ø°Ù Ø´Ø¯.")

    except Exception as e:
        logging.error(f"{ticker} - âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡: {e}")

# ---------------------- UPSERT ---------------------- #
if all_data:
    final_df = pd.concat(all_data, ignore_index=True)
    if 'BQ_Value' in final_df.columns:
        final_df = final_df.sort_values(by='BQ_Value', ascending=False)
    final_df = final_df.where(pd.notnull(final_df), None)

    logging.info(f"ğŸ“Š {len(final_df)} Ø±Ø¯ÛŒÙ Ø¨Ø±Ø§ÛŒ UPSERT Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.")

    try:
        engine = create_engine(DB_URL_SYNC)
        # âœ… Ø§Ú¯Ø± Ø¯Ø± DB Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø±Ø§ Ø±ÙˆÛŒ (inscode, date) Ø³Ø§Ø®ØªÙ‡â€ŒØ§ÛŒ:
        conflict_cols = ["inscode", "date"]   # â† ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        # Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ø¨Ø§ ØªÛŒÚ©Ø± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒ: conflict_cols = ["stock_ticker", "date"]

        records = final_df.to_dict(orient="records")
        with engine.begin() as connection:
            md = MetaData()
            quote = Table("quote", md, autoload_with=connection)
            update_cols = [c.name for c in quote.columns if c.name not in conflict_cols]

            chunk_size = 1000
            for i in range(0, len(records), chunk_size):
                chunk = records[i:i+chunk_size]
                stmt = pg_insert(quote).values(chunk)
                update_map = {c: getattr(stmt.excluded, c) for c in update_cols}
                upsert_stmt = stmt.on_conflict_do_update(
                    index_elements=conflict_cols,
                    set_=update_map
                )
                connection.execute(upsert_stmt)

        logging.info("âœ… UPSERT Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± UPSERT Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
else:
    logging.warning("âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª.")

# ---------------------- Ø¨Ø³ØªÙ† Ø§ØªØµØ§Ù„ ---------------------- #
cursor.close()
conn.close()
